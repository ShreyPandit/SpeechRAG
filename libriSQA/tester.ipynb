{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load from local files\n",
    "dataset = load_dataset('json', \n",
    "                      data_files={\n",
    "                          'data': 'config/LibriSQA-PartI-flac.json'\n",
    "                      })\n",
    "\n",
    "\n",
    "dataset = dataset['data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'duration', 'question', 'answer', 'speech_path'],\n",
      "    num_rows: 2620\n",
      "})\n",
      "Number of examples: 2620\n"
     ]
    }
   ],
   "source": [
    "# Print basic information about the dataset\n",
    "print(dataset)\n",
    "print(f\"Number of examples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoFeatureExtractor, MimiModel\n",
    "import os\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import List, Dict, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Global variables for model and feature extractor\n",
    "model = None\n",
    "feature_extractor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_mimi_model():\n",
    "    \"\"\"Initialize MIMI model and feature extractor\"\"\"\n",
    "    global model, feature_extractor\n",
    "    if model is None or feature_extractor is None:\n",
    "        model = MimiModel.from_pretrained(\"kyutai/mimi\")\n",
    "        feature_extractor = AutoFeatureExtractor.from_pretrained(\"kyutai/mimi\")\n",
    "    return model, feature_extractor\n",
    "\n",
    "@dataclass\n",
    "class AudioChunkInfo:\n",
    "    \"\"\"Class to hold information about an audio chunk\"\"\"\n",
    "    chunk_number: int\n",
    "    start_time: float  # in seconds\n",
    "    end_time: float    # in seconds\n",
    "    duration: float    # in seconds\n",
    "    file_path: str\n",
    "\n",
    "@dataclass\n",
    "class SimilarityResult:\n",
    "    \"\"\"Class to hold similarity computation results\"\"\"\n",
    "    chunk_id: str\n",
    "    similarity_score: float\n",
    "    chunk_start_time: float\n",
    "    chunk_end_time: float\n",
    "    embedding_path: str\n",
    "\n",
    "def extract_mimi_embeddings(audio_path):\n",
    "    \"\"\"Extract MIMI embeddings from an audio file.\"\"\"\n",
    "    global model, feature_extractor\n",
    "    \n",
    "    # Initialize model if not already done\n",
    "    if model is None or feature_extractor is None:\n",
    "        model, feature_extractor = initialize_mimi_model()\n",
    "    \n",
    "    try:\n",
    "        # Load the audio file\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Resample if necessary\n",
    "        if sample_rate != feature_extractor.sampling_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, feature_extractor.sampling_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Prepare inputs\n",
    "        inputs = feature_extractor(\n",
    "            raw_audio=waveform.squeeze().numpy(),\n",
    "            sampling_rate=feature_extractor.sampling_rate,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            # Get encoder outputs\n",
    "            encoder_outputs = model.encode(inputs[\"input_values\"])\n",
    "            embeddings = encoder_outputs.audio_codes.float()\n",
    "            \n",
    "            # Convert to fixed-size embedding by taking mean across time dimension\n",
    "            if len(embeddings.shape) == 3:\n",
    "                embeddings = torch.mean(embeddings, dim=1)\n",
    "            elif len(embeddings.shape) == 2:\n",
    "                embeddings = torch.mean(embeddings, dim=0, keepdim=True)\n",
    "                \n",
    "            # Normalize the embeddings\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=-1)\n",
    "            \n",
    "        return embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in extract_mimi_embeddings: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingSimilarityCalculator:\n",
    "    \"\"\"Handles similarity computations between embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings_dir: str):\n",
    "        self.embeddings_dir = Path(embeddings_dir)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def load_embedding(self, embedding_path: Union[str, Path]) -> torch.Tensor:\n",
    "        \"\"\"Load embedding from file and process it\"\"\"\n",
    "        try:\n",
    "            embedding = torch.load(embedding_path, map_location=self.device)\n",
    "            \n",
    "            # Convert to float if needed\n",
    "            if not embedding.is_floating_point():\n",
    "                embedding = embedding.float()\n",
    "            \n",
    "            # Get to 2D tensor shape (1, features)\n",
    "            if len(embedding.shape) == 3:  # (batch, sequence, features)\n",
    "                embedding = embedding.mean(dim=1)  # Average over sequence dimension\n",
    "            if len(embedding.shape) == 2:  # (sequence, features)\n",
    "                embedding = embedding.mean(dim=0, keepdim=True)  # Average to single vector\n",
    "            if len(embedding.shape) == 1:  # (features,)\n",
    "                embedding = embedding.unsqueeze(0)  # Add batch dimension\n",
    "                \n",
    "            return embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading embedding from {embedding_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def process_embedding_for_comparison(self, embedding: torch.Tensor, target_dim: int) -> torch.Tensor:\n",
    "        \"\"\"Process embedding to match target dimension\"\"\"\n",
    "        if embedding.shape[1] != target_dim:\n",
    "            # Use linear interpolation to resize to target dimension\n",
    "            embedding = F.interpolate(\n",
    "                embedding.unsqueeze(1),  # Add channel dimension\n",
    "                size=target_dim,\n",
    "                mode='linear',\n",
    "                align_corners=False\n",
    "            ).squeeze(1)  # Remove channel dimension\n",
    "        return embedding\n",
    "\n",
    "    def compute_cosine_similarity(self, \n",
    "                                query_embedding: torch.Tensor,\n",
    "                                chunk_embedding: torch.Tensor) -> float:\n",
    "        \"\"\"Compute cosine similarity between query and chunk embeddings\"\"\"\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                # Ensure both are 2D\n",
    "                if len(query_embedding.shape) == 1:\n",
    "                    query_embedding = query_embedding.unsqueeze(0)\n",
    "                if len(chunk_embedding.shape) == 1:\n",
    "                    chunk_embedding = chunk_embedding.unsqueeze(0)\n",
    "\n",
    "                # Get the minimum dimension\n",
    "                min_dim = min(query_embedding.shape[1], chunk_embedding.shape[1])\n",
    "                \n",
    "                # Resize both embeddings to the minimum dimension\n",
    "                query_embedding = self.process_embedding_for_comparison(query_embedding, min_dim)\n",
    "                chunk_embedding = self.process_embedding_for_comparison(chunk_embedding, min_dim)\n",
    "                \n",
    "                # Normalize embeddings\n",
    "                query_embedding = F.normalize(query_embedding, p=2, dim=1)\n",
    "                chunk_embedding = F.normalize(chunk_embedding, p=2, dim=1)\n",
    "                \n",
    "                # Compute similarity\n",
    "                similarity = F.cosine_similarity(query_embedding, chunk_embedding, dim=1)\n",
    "                \n",
    "                return similarity.item()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in compute_cosine_similarity: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def find_most_similar_chunks(self,\n",
    "                               query_path: str,\n",
    "                               top_k: int = 1,\n",
    "                               metadata_path: Optional[str] = None) -> List[SimilarityResult]:\n",
    "        \"\"\"Find the most similar chunks to a query\"\"\"\n",
    "        # Load query embedding\n",
    "        query_embedding = self.load_embedding(query_path)\n",
    "        \n",
    "        # Load metadata if available\n",
    "        chunk_metadata = {}\n",
    "        if metadata_path and Path(metadata_path).exists():\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                chunk_metadata = json.load(f)\n",
    "        \n",
    "        # Process all chunk embeddings\n",
    "        results = []\n",
    "        chunk_paths = sorted(self.embeddings_dir.glob(\"chunk_*.pt\"))\n",
    "        \n",
    "        for chunk_path in tqdm(chunk_paths, desc=\"Processing chunks\"):\n",
    "            try:\n",
    "                metadata = chunk_metadata.get(chunk_path.stem, {})\n",
    "                chunk_embedding = self.load_embedding(chunk_path)\n",
    "                similarity_score = self.compute_cosine_similarity(\n",
    "                    query_embedding,\n",
    "                    chunk_embedding\n",
    "                )\n",
    "                \n",
    "                result = SimilarityResult(\n",
    "                    chunk_id=chunk_path.stem,\n",
    "                    similarity_score=similarity_score,\n",
    "                    chunk_start_time=metadata.get('start_time', 0.0),\n",
    "                    chunk_end_time=metadata.get('end_time', 0.0),\n",
    "                    embedding_path=str(chunk_path)\n",
    "                )\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing chunk {chunk_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Sort by similarity score and get top-k\n",
    "        results.sort(key=lambda x: x.similarity_score, reverse=True)\n",
    "        return results[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class ChunkInfo:\n",
    "    chunk_number: int\n",
    "    file_path: str\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "    duration: float\n",
    "\n",
    "@dataclass\n",
    "class SimilarityResult:\n",
    "    chunk_id: str\n",
    "    similarity_score: float\n",
    "    chunk_start_time: float\n",
    "    chunk_end_time: float\n",
    "    embedding_path: str\n",
    "\n",
    "class FLACSplitter:\n",
    "    def __init__(self, chunk_duration: int, output_dir: str, min_chunk_duration: int):\n",
    "        self.chunk_duration = chunk_duration\n",
    "        self.output_dir = output_dir\n",
    "        self.min_chunk_duration = min_chunk_duration\n",
    "\n",
    "    def split_audio(self, input_file: str) -> List[ChunkInfo]:\n",
    "        \"\"\"Split FLAC file into chunks\"\"\"\n",
    "        # Read the FLAC file\n",
    "        data, samplerate = sf.read(input_file)\n",
    "        duration = len(data) / samplerate\n",
    "        chunk_infos = []\n",
    "        \n",
    "        # Calculate number of chunks\n",
    "        num_chunks = int(duration // self.chunk_duration)\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        for i in range(num_chunks):\n",
    "            start_time = i * self.chunk_duration\n",
    "            end_time = start_time + self.chunk_duration\n",
    "            \n",
    "            # Convert time to samples\n",
    "            start_sample = int(start_time * samplerate)\n",
    "            end_sample = int(end_time * samplerate)\n",
    "            \n",
    "            # Get chunk data\n",
    "            chunk_data = data[start_sample:end_sample]\n",
    "            \n",
    "            # Skip if chunk is too short\n",
    "            if len(chunk_data) / samplerate < self.min_chunk_duration:\n",
    "                continue\n",
    "                \n",
    "            # Save chunk\n",
    "            chunk_path = os.path.join(self.output_dir, f\"chunk_{i:03d}.flac\")\n",
    "            sf.write(chunk_path, chunk_data, samplerate)\n",
    "            \n",
    "            # Create chunk info\n",
    "            chunk_info = ChunkInfo(\n",
    "                chunk_number=i,\n",
    "                file_path=chunk_path,\n",
    "                start_time=start_time,\n",
    "                end_time=end_time,\n",
    "                duration=len(chunk_data) / samplerate\n",
    "            )\n",
    "            chunk_infos.append(chunk_info)\n",
    "            \n",
    "        return chunk_infos\n",
    "\n",
    "class AudioMetadata:\n",
    "    @staticmethod\n",
    "    def save_chunk_metadata(chunk_infos: List[ChunkInfo], output_dir: str):\n",
    "        \"\"\"Save metadata about chunks to JSON file\"\"\"\n",
    "        metadata = {\n",
    "            \"chunks\": [\n",
    "                {\n",
    "                    \"chunk_number\": info.chunk_number,\n",
    "                    \"file_path\": info.file_path,\n",
    "                    \"start_time\": info.start_time,\n",
    "                    \"end_time\": info.end_time,\n",
    "                    \"duration\": info.duration\n",
    "                }\n",
    "                for info in chunk_infos\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        output_path = os.path.join(output_dir, \"chunk_metadata.json\")\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "class SimilarityVisualizer:\n",
    "    @staticmethod\n",
    "    def save_results(results: List[SimilarityResult], output_path: str):\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(\"Similarity Results:\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            \n",
    "            for i, result in enumerate(results, 1):\n",
    "                f.write(f\"Rank {i}:\\n\")\n",
    "                f.write(f\"  Chunk ID: {result.chunk_id}\\n\")\n",
    "                f.write(f\"  Similarity Score: {result.similarity_score:.4f}\\n\")\n",
    "                f.write(f\"  Time Range: {result.chunk_start_time:.2f}s - \"\n",
    "                       f\"{result.chunk_end_time:.2f}s\\n\")\n",
    "                f.write(f\"  Embedding: {result.embedding_path}\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_similarities(results: List[SimilarityResult], output_path: str):\n",
    "        chunk_ids = [r.chunk_id for r in results]\n",
    "        scores = [r.similarity_score for r in results]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x=range(len(chunk_ids)), y=scores)\n",
    "        plt.title(\"Chunk Similarity Scores\")\n",
    "        plt.xlabel(\"Chunk ID\")\n",
    "        plt.ylabel(\"Cosine Similarity\")\n",
    "        plt.xticks(range(len(chunk_ids)), chunk_ids, rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path)\n",
    "        plt.close()\n",
    "\n",
    "def process_audio_files():\n",
    "    \"\"\"Process main audio file and extract embeddings\"\"\"\n",
    "    input_file = \"/mnt/data/ashwin/SpeechRAG/libriSQA/twomerged/61_672_merged.flac\"  # Update with your FLAC file path\n",
    "    output_dir = \"/mnt/data/ashwin/SpeechRAG/libriSQA/processed_audio\"\n",
    "    embeddings_dir = \"/mnt/data/ashwin/SpeechRAG/libriSQA/embeddings/chunks_embedding\"\n",
    "    chunk_duration = 15\n",
    "    min_chunk_duration = 5\n",
    "\n",
    "    try:\n",
    "        # Initialize MIMI model\n",
    "        initialize_mimi_model()\n",
    "        \n",
    "        # Initialize splitter\n",
    "        splitter = FLACSplitter(\n",
    "            chunk_duration=chunk_duration,\n",
    "            output_dir=output_dir,\n",
    "            min_chunk_duration=min_chunk_duration\n",
    "        )\n",
    "        \n",
    "        # Split audio\n",
    "        chunk_infos = splitter.split_audio(input_file)\n",
    "        \n",
    "        # Save metadata and create embeddings directory\n",
    "        if chunk_infos:\n",
    "            output_dir = os.path.dirname(chunk_infos[0].file_path)\n",
    "            AudioMetadata.save_chunk_metadata(chunk_infos, output_dir)\n",
    "            Path(embeddings_dir).mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Process each chunk\n",
    "            for chunk_info in tqdm(chunk_infos, desc=\"Extracting embeddings\"):\n",
    "                try:\n",
    "                    embeddings = extract_mimi_embeddings(chunk_info.file_path)\n",
    "                    output_path = os.path.join(embeddings_dir, f\"chunk_{chunk_info.chunk_number:03d}.pt\")\n",
    "                    torch.save(embeddings, output_path)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing chunk {chunk_info.chunk_number}: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "        logger.info(\"Processing completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_query_audio(query_audio_path: str, output_path: str):\n",
    "    \"\"\"Process query audio and save embeddings\"\"\"\n",
    "    try:\n",
    "        initialize_mimi_model()\n",
    "        embeddings = extract_mimi_embeddings(query_audio_path)\n",
    "        torch.save(embeddings, output_path)\n",
    "        logger.info(f\"Query embeddings saved to {output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing query audio: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def find_similar_chunks():\n",
    "    \"\"\"Find chunks similar to query\"\"\"\n",
    "    output_dir = \"/mnt/data/ashwin/SpeechRAG/libriSQA/results\"\n",
    "    embeddings_dir = \"/mnt/data/ashwin/SpeechRAG/libriSQA/embeddings/chunks_embedding\"\n",
    "    query_path = \"/mnt/data/ashwin/SpeechRAG/libriSQA/embeddings/question_embedding.pt\"\n",
    "    # metadata_path = \"/path/to/chunk_metadata.json\"\n",
    "    top_k = 20\n",
    "\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    calculator = EmbeddingSimilarityCalculator(embeddings_dir)\n",
    "    results = calculator.find_most_similar_chunks(\n",
    "        query_path,\n",
    "        top_k=top_k,\n",
    "        # metadata_path=metadata_path\n",
    "    )\n",
    "    \n",
    "    SimilarityVisualizer.save_results(\n",
    "        results,\n",
    "        output_dir / \"similarity_results.txt\"\n",
    "    )\n",
    "    \n",
    "    SimilarityVisualizer.plot_similarities(\n",
    "        results,\n",
    "        output_dir / \"similarity_plot.png\"\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Similarity search completed successfully!\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e188082bbbdf4164bcf39ba860a8bc53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7abd3a4fde4951bf3a5e1428870b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/385M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/ashwin/miniconda3/envs/SLT/lib/python3.8/site-packages/transformers/models/mimi/modeling_mimi.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07243aa34244dc0869d37703063a92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing main audio file...\n",
      "Extracting embeddings: 100%|██████████| 65/65 [00:32<00:00,  2.00it/s]\n",
      "INFO:__main__:Processing completed successfully!\n",
      "INFO:__main__:Processing query audio with MIMI embeddings...\n",
      "INFO:__main__:Query embeddings saved to /mnt/data/ashwin/SpeechRAG/libriSQA/embeddings/question_embedding.pt\n"
     ]
    }
   ],
   "source": [
    "initialize_mimi_model()\n",
    "\n",
    "# First process the main audio file\n",
    "logger.info(\"Processing main audio file...\")\n",
    "process_audio_files()\n",
    "\n",
    "# Then process the query audio with MIMI\n",
    "logger.info(\"Processing query audio with MIMI embeddings...\")\n",
    "query_audio = \"/mnt/data/ashwin/SpeechRAG/libriSQA/question_long_news.mp3\"\n",
    "query_embedding_path = \"/mnt/data/ashwin/SpeechRAG/libriSQA/embeddings/question_embedding.pt\"\n",
    "\n",
    "# Extract MIMI embeddings for query\n",
    "embeddings = extract_mimi_embeddings(query_audio)\n",
    "torch.save(embeddings, query_embedding_path)\n",
    "logger.info(f\"Query embeddings saved to {query_embedding_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
